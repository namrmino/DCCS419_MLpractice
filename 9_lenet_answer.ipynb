{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"9_lenet_answer.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"75fda7912ec448468c2ffd64296743e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e667a945420b4e18a80ad8f6f628bba1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_00ea7a8727d841eaaa4a530e8441da70","IPY_MODEL_12a532dfb4234e71850552f9674fccd8"]}},"e667a945420b4e18a80ad8f6f628bba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00ea7a8727d841eaaa4a530e8441da70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_09e2437c5a7143b180f0f04008014884","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_663fe9690ffe4047ad6cf9c7cf373e99"}},"12a532dfb4234e71850552f9674fccd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_08b37741ff21444380cc4b6ade82a884","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:20&lt;00:00, 2642320.57it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f2661f478b54f66a577b9e84b89c5b6"}},"09e2437c5a7143b180f0f04008014884":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"663fe9690ffe4047ad6cf9c7cf373e99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08b37741ff21444380cc4b6ade82a884":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f2661f478b54f66a577b9e84b89c5b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b59cd1354d4742a98a526dbb63156a23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3ec8a9a9af6143778de0e79dd2192742","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e4ca3cb52a324e59af83e6b802bdb328","IPY_MODEL_598b5216dd094d7ab743155abeed34ba"]}},"3ec8a9a9af6143778de0e79dd2192742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4ca3cb52a324e59af83e6b802bdb328":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8e726a0a8a60433592c3f1f9fbb0332f","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cfe6fe6e17349ce87c59f07b68f6e48"}},"598b5216dd094d7ab743155abeed34ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_df1ffbddfe4145be9994a4af6c5a9818","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/28881 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f9687021713428ba9f197665c2e1e3a"}},"8e726a0a8a60433592c3f1f9fbb0332f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2cfe6fe6e17349ce87c59f07b68f6e48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df1ffbddfe4145be9994a4af6c5a9818":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3f9687021713428ba9f197665c2e1e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1e01fd2a9d846d29686015e9e57ce5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_78e8d0356aec48559c817e8076156d97","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_99b48aed53264a2d8bace894af98e9cf","IPY_MODEL_e9ebb70c03af49d088b5c449c0c6eda8"]}},"78e8d0356aec48559c817e8076156d97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99b48aed53264a2d8bace894af98e9cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ebd4ae9a0881454eb0ea27e345861d7c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4604f347e9de4687932277821ad28d6a"}},"e9ebb70c03af49d088b5c449c0c6eda8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4bccc8b555ff4c479e28f3d766e1d72e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:01&lt;00:00, 1332660.89it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea2663abbf63492dae940951008cfc66"}},"ebd4ae9a0881454eb0ea27e345861d7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4604f347e9de4687932277821ad28d6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4bccc8b555ff4c479e28f3d766e1d72e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ea2663abbf63492dae940951008cfc66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a46843586465497bb7484ba40679fdec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2ffa383e0071441fb70ade6af0fc35b6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6c07e6060ca84642801d62d8fac14665","IPY_MODEL_1c11e91fb5c044cd8cafb72f161494ad"]}},"2ffa383e0071441fb70ade6af0fc35b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c07e6060ca84642801d62d8fac14665":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_85e75ffe747c429fbcf1cd73eebdcd2b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e09310ae35fc4238b207057f72e888ee"}},"1c11e91fb5c044cd8cafb72f161494ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67afa04d49d942a58dbcec109fd2ed9b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8192/? [00:00&lt;00:00, 19664.90it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2e423bd2d4c488aa72c03ad92bddb89"}},"85e75ffe747c429fbcf1cd73eebdcd2b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e09310ae35fc4238b207057f72e888ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67afa04d49d942a58dbcec109fd2ed9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d2e423bd2d4c488aa72c03ad92bddb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"2tCipQ-4nGpr","colab_type":"text"},"source":["# Convolutional Neural Networks: MNIST\n","\n","In this notebook, you will:\n","\n","- Implement helper functions that you will use when implementing a PyTorch model\n","- Implement a fully functioning ConvNet using PyTorch \n","\n","**After this assignment you will be able to:**\n","\n","- Build and train a ConvNet in PyTorch for a classification problem \n","\n","We assume here that you are already familiar with PyTorch. "]},{"cell_type":"code","metadata":{"id":"aatpvdbgms50","colab_type":"code","outputId":"774a71d9-d681-4f1a-a777-4e1c1e86aab1","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1590377080266,"user_tz":-540,"elapsed":5319,"user":{"displayName":"‍서경은[ 대학원박사과정수료연구(재학) / 컴퓨터정보학과 ]","photoUrl":"","userId":"17168298096082030735"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('using device:', device)\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,1), 'MB')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["using device: cuda\n","Tesla K80\n","Memory usage:\n","Allocated: 0.0 MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0BKpoUNNnQxA","colab_type":"text"},"source":["## 1. PyTorch model\n","\n","### 1.1. Load dataset (MNIST) \n","\n","Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n","\n","We will start by loading in the packages. \n","\n","The MNIST Dataset is located at http://yann.lecun.com/exdb/mnist/. Each image has $28 \\times 28$ dimension.\n","\n","-train-images-idx3-ubyte.gz:  training set images (9912422 bytes) including 55000 examples <br>\n","-train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) <br>\n","-t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) including 10000 examples <br>\n","-t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)"]},{"cell_type":"code","metadata":{"id":"zFF0e5udms54","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360,"referenced_widgets":["75fda7912ec448468c2ffd64296743e6","e667a945420b4e18a80ad8f6f628bba1","00ea7a8727d841eaaa4a530e8441da70","12a532dfb4234e71850552f9674fccd8","09e2437c5a7143b180f0f04008014884","663fe9690ffe4047ad6cf9c7cf373e99","08b37741ff21444380cc4b6ade82a884","7f2661f478b54f66a577b9e84b89c5b6","b59cd1354d4742a98a526dbb63156a23","3ec8a9a9af6143778de0e79dd2192742","e4ca3cb52a324e59af83e6b802bdb328","598b5216dd094d7ab743155abeed34ba","8e726a0a8a60433592c3f1f9fbb0332f","2cfe6fe6e17349ce87c59f07b68f6e48","df1ffbddfe4145be9994a4af6c5a9818","3f9687021713428ba9f197665c2e1e3a","c1e01fd2a9d846d29686015e9e57ce5d","78e8d0356aec48559c817e8076156d97","99b48aed53264a2d8bace894af98e9cf","e9ebb70c03af49d088b5c449c0c6eda8","ebd4ae9a0881454eb0ea27e345861d7c","4604f347e9de4687932277821ad28d6a","4bccc8b555ff4c479e28f3d766e1d72e","ea2663abbf63492dae940951008cfc66","a46843586465497bb7484ba40679fdec","2ffa383e0071441fb70ade6af0fc35b6","6c07e6060ca84642801d62d8fac14665","1c11e91fb5c044cd8cafb72f161494ad","85e75ffe747c429fbcf1cd73eebdcd2b","e09310ae35fc4238b207057f72e888ee","67afa04d49d942a58dbcec109fd2ed9b","d2e423bd2d4c488aa72c03ad92bddb89"]},"outputId":"212c9309-6e30-4f84-b872-a61cd32c1d31","executionInfo":{"status":"ok","timestamp":1590377086719,"user_tz":-540,"elapsed":11769,"user":{"displayName":"‍서경은[ 대학원박사과정수료연구(재학) / 컴퓨터정보학과 ]","photoUrl":"","userId":"17168298096082030735"}}},"source":["train_dataset = datasets.MNIST( root='./mnist_data/',\n","                              train=True,\n","                              transform=transforms.ToTensor(),\n","                              download=True)\n","test_dataset = datasets.MNIST( root='./mnist_data/',\n","                             train = False,\n","                             transform=transforms.ToTensor())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75fda7912ec448468c2ffd64296743e6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b59cd1354d4742a98a526dbb63156a23","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1e01fd2a9d846d29686015e9e57ce5d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a46843586465497bb7484ba40679fdec","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rwkQNi5foOTO","colab_type":"text"},"source":["### 1.1. Prepare dataset\n","\n","Let's prepare the dataset using **torch.tuils.data.Dataloader**\n","\n","PyTorch provides DataLoader for the input data (training/testing) that will be fed into the model when training/testing the model.\n","\n","**Exercise**: Implement the function below to prepare the training/testing data. You can define the number of examples for the moment and shuffle the order of examples by setting batch_size and shuffle.\n","\n","**The constructor arguments of a DataLoader :**\n","```python\n","torch.utils.data.DataLoader (dataset, batch_size=1, shuffle=False, sampler=None,\n","                            batch_sampler=None, num_workers=0, collate_fn=None,\n","                            pin_memory=False, drop_last=False, timeout=0,\n","                            worker_init_fn=None)\n","```"]},{"cell_type":"code","metadata":{"id":"BDNXScGRoIEb","colab_type":"code","colab":{}},"source":["### Set the batch_size \n","batch_size = 64\n","\n","### START CODE HERE ### (≈1 lines)\n","#Use torch.utils.data.DataLoader() with bath_size 64, shuffle = True \n","train_loader = torch.utils.data.DataLoader( dataset = train_dataset,\n","                                           batch_size = batch_size,\n","                                           shuffle = True)\n","### ENd CODE HERE ###  \n","\n","test_loader = torch.utils.data.DataLoader( dataset=test_dataset,\n","                                          batch_size = batch_size,\n","                                          shuffle=False)\n","\n","# train_loader has (x, y) where x.size()=[64, 1, 28, 28] and y.size()=[64]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpdTftvIpRFs","colab_type":"text"},"source":["### 1.2. Forward propagation\n","\n","In PyTorch, there are built-in functions that carry out the convolution steps for you.\n","\n","- **torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):** Applies a 2D convolution over an input signal composed of several input planes.  \n","\n","- **torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):** applies a 2D max pooling over an input signal composed of several input planes.\n","\n","- **torch.nn.Linear(in_features, out_features, bias=True):** Applies a linear transformation to the incoming data\n","\n","- **torch.nn.functional.relu(input, inplace=False):** Applies the rectified linear unit function element-wise \n","\n","- **torch.nn.functional.softmax(dim=None):** torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n","\n","- **torch.flatten(input, start_dim=0, end_dim=-1) :** Flattens a contiguous range of dims in a tensor.\n","\n","**For more details:**\n","\n","torch.nn : https://pytorch.org/docs/stable/nn.html\n","\n","torch.functional : https://pytorch.org/docs/stable/nn.functional.html\n","\n","torch.flatten : https://pytorch.org/docs/master/generated/torch.flatten.html\n","\n","\n","**Exercise**: \n","\n","Implement the `NNModel` class below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n"," "]},{"cell_type":"code","metadata":{"id":"9bDY8CEgms6f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"e863a4f1-47f4-4ae5-e43c-9be655793922","executionInfo":{"status":"ok","timestamp":1590377086721,"user_tz":-540,"elapsed":11766,"user":{"displayName":"‍서경은[ 대학원박사과정수료연구(재학) / 컴퓨터정보학과 ]","photoUrl":"","userId":"17168298096082030735"}}},"source":["\"\"\"\n","Implements forward propabation for the CNN model:\n","CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n","\n","\"\"\"\n","class NNModel(torch.nn.Module):\n","    def __init__(self):\n","        super(NNModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5, padding=(2,2)) # input channel, output channels, and filter size\n","        self.pool1 = nn.MaxPool2d(2, stride=2)\n","        \n","        ### START CODE HERE ###\n","        # Define the Convolution layer(Condv2d) with (32:input channel, 64:output channels, 5: filter size) and the 2 padding. \n","        self.conv2 = nn.Conv2d(32, 64, 5, padding=(2,2))  \n","        # Define the Maxpooling layer(MaxPool2d) with (2:filter size, 2:stride)\n","        self.pool2 = nn.MaxPool2d(2, stride=2)\n","        # Define the fully-conneted layer(Linear) with (3136:input channel, 1024:output channel)\n","        self.fc1 = nn.Linear(3136,1024)\n","        ### END CODE HERE ###\n","        self.fc2 = nn.Linear(1024,10)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","            Arguments:\n","            X -- input dataset placeholder, of shape (input size, number of examples)\n","            parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n","                          the shapes are given in initialize_parameters\n","\n","            Returns:\n","            F.softmax(x) -- the softmax output of the last LINEAR unit\n","        \"\"\"\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.pool1(x)\n","        ### START CODE HERE ###\n","        # Load sceond Convoultion layer(conv2d) \n","        x = self.conv2(x)\n","        # Load Relu function  \n","        x = F.relu(x)\n","        # Load sceond Maxpooling layer(MaxPool2d) \n","        x = self.pool2(x)\n","        # Flatten each example into a 1D vector: (?,7,7,64)->(?, 3136)\n","        x = torch.flatten(x, 1)\n","        # Load first fully-connted layer \n","        x = self.fc1(x) \n","        ### END CODE HERE ### \n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return F.softmax(x)    \n","    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RvmWxnal0hWA","colab_type":"text"},"source":["### 1.3. Build model\n","\n","Finally you will merge the helper functions you implemented above to build and train a model.  \n","\n","**Exercise**: Complete the function below. \n","\n","The model below should:\n","\n","- Define the cost function with **torch.nn.CrossEntropyLoss()**    \n","> torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') \n","> **For more details:** https://pytorch.org/docs/stable/nn.html\n","\n","- Create optmizization function with **torch.optim.SGD()** \n","> torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)     \n","> **For more details:** https://pytorch.org/docs/stable/optim.html\n","- hint : For getting params for optmizer, use the following code\n","```python\n","params = model.parameters()\n","```\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"akEyMJMxms6i","colab_type":"code","colab":{}},"source":["model = NNModel()\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","\n","model.apply(weights_init)\n","\n","### START CODE HERE ###  \n","# Define the cost function with cross entropy      \n","criterion = nn.CrossEntropyLoss() \n","# Create optimizer \n","# Get model paramters \n","params = model.parameters()\n","# Define the optimizer (using SGD) with learing rate=0.01 and momentum=0.5\n","optimizer = optim.SGD(params, lr=0.01, momentum=0.5)\n","### END CODE HERE ###\n","\n","model = model.to(device)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"701_v22n3KxE","colab_type":"text"},"source":["## 2. Train and test model\n","\n","Finally you will create training and testing function with a loop for each mini-batch.\n"]},{"cell_type":"markdown","metadata":{"id":"I-cfYlJx4ig1","colab_type":"text"},"source":["### 2.1 Train function\n","\n","Implement the function to train the CNN model.\n","\n","**Exercise**: Complete the function below. \n","\n","- copy a tensor on the CPU to the GPU \n","- call the CNN model \n","- initialize the optimizer  \n","- compute the cost  \n","- compute the backward propagation\n","- update parameters with the optimizer \n","\n","**Hint**\n","- copy a tensor on the CPU to the GPU\n","         # copy a tensor to the CPU \n","         data = data.to(\"cpu\")\n","         # copy a tensor to the GPU \n","         data = data.to(\"cuda\") \n","- call the CNN model : call the predefined CNN model funciton    \n","- initialize optimizer \n","        optimizer.zero_grad() \n","- compute the cost : call the predifined criterion function  \n","- compute the backward propagation   \n","        loss.backward()\n","- update parameters with the optimizer \n","        optimizer.step()\n","\n","\n","\n"," "]},{"cell_type":"code","metadata":{"id":"40y8dtLi24_0","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        \n","        ### START CODE HERE ###  \n","        # Copy \"data\" tensor to the GPU\n","        data = data.to(device)\n","        # Copy \"target\" tensor to the GPU\n","        target = target.to(device)\n","        # Call model function and input the data tensor \n","        output = model(data)\n","        # Initialize the optimizer  \n","        optimizer.zero_grad()\n","        # Compute the cost \n","        loss = criterion(output, target)\n","        # Compute the back-prop\n","        loss.backward()\n","        # Update prameters with the optmizer \n","        optimizer.step()\n","        ### END CODE HERE ###\n","        if batch_idx%100==0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, \n","                batch_idx*len(data), \n","                len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), \n","                loss))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqMzKBR_4xCq","colab_type":"text"},"source":["### 2.2 Test function"]},{"cell_type":"code","metadata":{"id":"OXIwfghqms6n","colab_type":"code","colab":{}},"source":["def test():\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss=0\n","        correct = 0\n","        for data, target in train_loader:\n","            data = data.to(device)\n","            target = target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output,target)\n","            pred = output.data.max(1,keepdim=True)[1]\n","            correct += pred.eq(target.data.view_as(pred)).sum()\n","    \n","        test_loss /=len(train_loader.dataset)\n","        print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","            test_loss, correct, \n","            len(train_loader.dataset),\n","            100. * correct / len(train_loader.dataset)))\n","\n","        test_loss=0\n","        correct = 0\n","        for data, target in test_loader:\n","            data = data.to(device)\n","            target = target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output,target)\n","            pred = output.data.max(1,keepdim=True)[1]\n","            correct += pred.eq(target.data.view_as(pred)).sum()\n","    \n","        test_loss /=len(test_loader.dataset)\n","        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, correct, \n","            len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xleeHtag4DKl","colab_type":"text"},"source":["### 2.3. Run the train/test function\n","\n","Run the following cell to train and test your model for 100 epochs.  "]},{"cell_type":"code","metadata":{"id":"C5McB7DF1D5a","colab_type":"code","outputId":"6b229544-5ff6-4247-e07e-3d92f558d6de","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(0,100):\n","    train(epoch)\n","    if epoch%100==0:\n","        test()\n","test()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302927\n","Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.302317\n","Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.302096\n","Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.301569\n","Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.301427\n","Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.301263\n","Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.300505\n","Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.300470\n","Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.299942\n","Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.298971\n","Train set: Average loss: 0.0359, Accuracy: 27519/60000 (46%)\n","Test set: Average loss: 0.0361, Accuracy: 4612/10000 (46%)\n","\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298952\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.297770\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295723\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.293531\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.292355\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.286599\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.252783\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.231227\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.065572\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.857095\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.806256\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.773802\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.678254\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.626612\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.597142\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.645501\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.524424\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.516163\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.525906\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.510958\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.556859\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.544024\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.602876\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.640671\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.582964\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.514161\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.517880\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.527556\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.546311\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.602662\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.582406\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.514522\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.575655\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.523941\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.486806\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.553350\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.500507\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.554089\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.493089\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.513789\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.536820\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.490690\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.545106\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.534482\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.503824\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.523438\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.547057\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.574254\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.573352\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.534235\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.508892\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.500069\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.486772\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.513363\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.521872\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.533855\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.495896\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.563252\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.496792\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.535679\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.519200\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.507259\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.511341\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.506556\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.519047\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.490099\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.485216\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.492267\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.488507\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.506345\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.511697\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.472763\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.510280\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.494581\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.481230\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.529270\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.546141\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.508138\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.473331\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.535163\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.498630\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.486659\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.509327\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.474042\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.502944\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.492893\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.500099\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.492648\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.510002\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.484669\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.489449\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.472415\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.491095\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.522360\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.543310\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.494266\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.510698\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.472698\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.488367\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.499027\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.471783\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.488148\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.472029\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 1.498868\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.473349\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.530574\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.478182\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 1.481135\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.478126\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.462983\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.467003\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.463743\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.481292\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 1.483373\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.491900\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 1.492567\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.499347\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 1.493207\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.474838\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 1.467906\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.468328\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 1.490505\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.510305\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 1.495080\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.482580\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.479080\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.462860\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 1.488821\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.463323\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 1.463466\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.482845\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 1.485707\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.493982\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 1.511253\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.492825\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 1.464202\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.486520\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 1.527224\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.516927\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 1.501277\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.508591\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 1.501191\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.483877\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 1.470285\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.477726\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 1.462819\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.498956\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 1.498817\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.463820\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 1.483293\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.463402\n","Train Epoch: 16 [6400/60000 (11%)]\tLoss: 1.477198\n","Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.471752\n","Train Epoch: 16 [19200/60000 (32%)]\tLoss: 1.481085\n","Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.461618\n","Train Epoch: 16 [32000/60000 (53%)]\tLoss: 1.491395\n","Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.477095\n","Train Epoch: 16 [44800/60000 (75%)]\tLoss: 1.481111\n","Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.469040\n","Train Epoch: 16 [57600/60000 (96%)]\tLoss: 1.463503\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.475673\n","Train Epoch: 17 [6400/60000 (11%)]\tLoss: 1.477025\n","Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.476426\n","Train Epoch: 17 [19200/60000 (32%)]\tLoss: 1.481978\n","Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.461505\n","Train Epoch: 17 [32000/60000 (53%)]\tLoss: 1.475055\n","Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.486789\n","Train Epoch: 17 [44800/60000 (75%)]\tLoss: 1.478340\n","Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.484424\n","Train Epoch: 17 [57600/60000 (96%)]\tLoss: 1.478318\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.483548\n","Train Epoch: 18 [6400/60000 (11%)]\tLoss: 1.512141\n","Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.485755\n","Train Epoch: 18 [19200/60000 (32%)]\tLoss: 1.484692\n","Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.464943\n","Train Epoch: 18 [32000/60000 (53%)]\tLoss: 1.471488\n","Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.497682\n","Train Epoch: 18 [44800/60000 (75%)]\tLoss: 1.489479\n","Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.484317\n","Train Epoch: 18 [57600/60000 (96%)]\tLoss: 1.473381\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.461662\n","Train Epoch: 19 [6400/60000 (11%)]\tLoss: 1.472803\n","Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.478476\n","Train Epoch: 19 [19200/60000 (32%)]\tLoss: 1.493212\n","Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.496985\n"],"name":"stdout"}]}]}